# -*- coding: utf-8 -*-
"""Gradiant_Descent_ownClass.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ownaMLqNt9kmCq3zxRczakvIj8qZptCu
"""

from sklearn.datasets import make_regression
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import cross_val_score

x,y = make_regression(n_samples=100, n_features=1, n_informative=1, n_targets=1,noise=20, random_state=13)

y

plt.scatter(x,y)

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,random_state=2)

from sklearn.linear_model import LinearRegression
lr = LinearRegression()

lr.fit(x_train,y_train)
print(lr.coef_)
print(lr.intercept_)

y_pred = lr.predict(x_test)
from sklearn.metrics import r2_score
r2_score(y_test,y_pred)

np.mean(cross_val_score(lr,x,y,scoring='r2',cv=10))

class GDRegressor:
  def __init__(self,learning_rate,epochs):
    self.m = 100
    self.b = -120
    self.lr = learning_rate
    self.epochs = epochs

  def fit(self,x,y):
    for i in range(self.epochs):
      loss_slope_b = -2* np.sum(y- self.m*x.ravel() - self.b)
      loss_slope_m = -2* np.sum((y- self.m*x.ravel() - self.b)*x.ravel())
      self.b = self.b -(self.lr * loss_slope_b)
      self.m = self.m -(self.lr * loss_slope_m)
    print(self.m,self.b)
  def predict(self,x):
    return self.m*x + self.b

gd = GDRegressor(0.001,100)

gd.fit(x_train,y_train)

y_pred = gd.predict(x_test)
from sklearn.metrics import r2_score
r2_score(y_test,y_pred)

